{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('combined_data.csv')\n",
    "df.drop('DATE',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('CSUSHPISA',axis=1).values\n",
    "y=df['CSUSHPISA'].values\n",
    "\n",
    "X_train=X[:int(len(X)*0.65)]\n",
    "X_test=X[int(len(X)*0.65):]\n",
    "\n",
    "y_train=y[:int(len(X)*0.65)]\n",
    "y_test=y[int(len(X)*0.65):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "scaler=scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 12)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model=Sequential()\n",
    "    for i in range(hp.Int('num_layers',2,5)):\n",
    "        model.add(Dense(units=hp.Int('units_'+str(i),\n",
    "                                     min_value=16,\n",
    "                                     max_value=512,\n",
    "                                     step=32),\n",
    "                                     activation='elu',))\n",
    "        model.add(Dropout(hp.Float('dropout', 0.1, 0.2, step=0.1, default=0.5)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "        hp.Choice('learning_rate',[0.0002])),\n",
    "        loss='mse',\n",
    "        # metrics=['mse']\n",
    "    )\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project hyperparam/parmeters/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from hyperparam/parmeters/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner=RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=30,\n",
    "    executions_per_trial=3,\n",
    "    directory='hyperparam',\n",
    "    project_name='parmeters'\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train,\n",
    "             epochs=400,\n",
    "             validation_data=(X_test, y_test),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in hyperparam/parmeters\n",
      "Showing 10 best trials\n",
      "Objective(name='val_loss', direction='min')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 48\n",
      "dropout: 0.2\n",
      "units_1: 464\n",
      "learning_rate: 0.0002\n",
      "units_2: 368\n",
      "units_3: 80\n",
      "units_4: 16\n",
      "Score: 40.34173901875814\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 496\n",
      "dropout: 0.2\n",
      "units_1: 368\n",
      "learning_rate: 0.0002\n",
      "units_2: 80\n",
      "units_3: 48\n",
      "units_4: 464\n",
      "Score: 41.9729118347168\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 336\n",
      "dropout: 0.1\n",
      "units_1: 464\n",
      "learning_rate: 0.0002\n",
      "units_2: 144\n",
      "units_3: 80\n",
      "units_4: 144\n",
      "Score: 43.29555892944336\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 48\n",
      "dropout: 0.1\n",
      "units_1: 336\n",
      "learning_rate: 0.0002\n",
      "units_2: 432\n",
      "units_3: 176\n",
      "units_4: 336\n",
      "Score: 43.30108642578125\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 112\n",
      "dropout: 0.2\n",
      "units_1: 368\n",
      "learning_rate: 0.0002\n",
      "units_2: 112\n",
      "units_3: 304\n",
      "units_4: 304\n",
      "Score: 44.39666239420573\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 368\n",
      "dropout: 0.1\n",
      "units_1: 208\n",
      "learning_rate: 0.0002\n",
      "units_2: 48\n",
      "units_3: 80\n",
      "units_4: 336\n",
      "Score: 45.49665451049805\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 144\n",
      "dropout: 0.1\n",
      "units_1: 176\n",
      "learning_rate: 0.0002\n",
      "units_2: 464\n",
      "units_3: 304\n",
      "units_4: 496\n",
      "Score: 47.10833485921224\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_0: 496\n",
      "dropout: 0.1\n",
      "units_1: 80\n",
      "learning_rate: 0.0002\n",
      "units_2: 400\n",
      "units_3: 272\n",
      "units_4: 496\n",
      "Score: 47.38164393107096\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 2\n",
      "units_0: 144\n",
      "dropout: 0.2\n",
      "units_1: 464\n",
      "learning_rate: 0.0002\n",
      "Score: 47.639102935791016\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units_0: 336\n",
      "dropout: 0.2\n",
      "units_1: 400\n",
      "learning_rate: 0.0002\n",
      "units_2: 80\n",
      "units_3: 16\n",
      "units_4: 336\n",
      "Score: 47.76451873779297\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units_0 48\n",
      "units_1 464\n",
      "units_2 368\n",
      "units_3 80\n",
      "units_4 16\n",
      "learning_rate 0.0002\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "for h_param in [f\"units_{i}\" for i in range(0,5)] + ['learning_rate']:\n",
    "    print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 1s 28ms/step - loss: 21671.4531 - val_loss: 35828.8242\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 21064.7871 - val_loss: 34941.0664\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 20436.7676 - val_loss: 33989.0625\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 19725.5117 - val_loss: 32933.0391\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18903.4688 - val_loss: 31720.5391\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 18020.8398 - val_loss: 30355.6973\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 16957.7988 - val_loss: 28829.2754\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 15699.3867 - val_loss: 27059.8535\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 14412.0264 - val_loss: 24997.6914\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 12950.9053 - val_loss: 22597.0566\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 11226.2812 - val_loss: 19822.2480\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 9189.9307 - val_loss: 16655.3730\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6907.6763 - val_loss: 13254.4365\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 5071.5835 - val_loss: 9751.7734\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3169.4353 - val_loss: 6469.2944\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1578.2565 - val_loss: 3760.5972\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 978.7122 - val_loss: 1992.8333\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 804.5499 - val_loss: 1206.6843\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 996.5818 - val_loss: 1042.4442\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 986.9761 - val_loss: 1053.0603\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 685.3503 - val_loss: 1263.9446\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 879.0602 - val_loss: 1514.6738\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 946.1166 - val_loss: 1602.3477\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 680.6820 - val_loss: 1454.8395\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 751.3640 - val_loss: 1262.3348\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 775.7586 - val_loss: 1204.2872\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 752.4313 - val_loss: 1223.4518\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 707.1429 - val_loss: 1229.6449\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 677.5056 - val_loss: 1287.7549\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 666.2917 - val_loss: 1306.8397\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 694.6149 - val_loss: 1243.2855\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 715.3836 - val_loss: 1064.7484\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 677.5010 - val_loss: 859.8701\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 667.8368 - val_loss: 750.4652\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 621.4954 - val_loss: 746.6885\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 652.6110 - val_loss: 834.1868\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 782.6003 - val_loss: 951.8139\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 654.7769 - val_loss: 960.2879\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 765.4943 - val_loss: 913.2946\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 713.9308 - val_loss: 873.3470\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 609.8276 - val_loss: 758.9926\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 658.7731 - val_loss: 652.8957\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 646.4227 - val_loss: 570.9092\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 654.2288 - val_loss: 561.6091\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 642.0590 - val_loss: 614.1885\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 661.1393 - val_loss: 652.6929\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 596.0547 - val_loss: 680.3223\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 790.5167 - val_loss: 655.6049\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 740.6451 - val_loss: 619.7455\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 745.3032 - val_loss: 576.7014\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 594.2786 - val_loss: 481.9977\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 715.7354 - val_loss: 456.4324\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 788.1740 - val_loss: 486.5101\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 539.6818 - val_loss: 497.1543\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 573.7247 - val_loss: 519.4847\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 531.5307 - val_loss: 514.5092\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 681.7959 - val_loss: 520.0263\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 711.2354 - val_loss: 451.1835\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 662.0967 - val_loss: 406.3712\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 553.9468 - val_loss: 375.1374\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 660.4476 - val_loss: 387.5733\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 582.3545 - val_loss: 436.4594\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 665.2201 - val_loss: 361.4091\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 563.8343 - val_loss: 260.4254\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 636.3900 - val_loss: 223.1310\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 687.4764 - val_loss: 308.6782\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 664.8543 - val_loss: 313.8039\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 559.7761 - val_loss: 239.1907\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 653.0352 - val_loss: 249.8267\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 621.1010 - val_loss: 310.3641\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 499.6177 - val_loss: 276.0700\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 570.2881 - val_loss: 238.3010\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 575.3121 - val_loss: 282.2839\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 611.6336 - val_loss: 331.7784\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 586.8932 - val_loss: 265.0342\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 537.7396 - val_loss: 222.6110\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 583.5734 - val_loss: 233.8286\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 655.6740 - val_loss: 244.2785\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 649.4229 - val_loss: 256.8238\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 539.7635 - val_loss: 296.2498\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 577.6568 - val_loss: 272.2867\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 622.2489 - val_loss: 250.7999\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 528.6336 - val_loss: 183.6097\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 583.7273 - val_loss: 141.6998\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 494.6774 - val_loss: 122.5569\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 578.4336 - val_loss: 132.3486\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 537.2655 - val_loss: 188.8276\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 588.9766 - val_loss: 189.7443\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 550.6840 - val_loss: 172.6208\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 535.6318 - val_loss: 154.6685\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 503.6838 - val_loss: 131.7382\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 574.0069 - val_loss: 122.0724\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 548.9232 - val_loss: 131.2835\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 569.6728 - val_loss: 145.5545\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 584.9515 - val_loss: 104.2341\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 469.1281 - val_loss: 135.4858\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 516.3951 - val_loss: 160.2597\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 451.2068 - val_loss: 162.5873\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 582.2349 - val_loss: 113.0941\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 482.0513 - val_loss: 70.4747\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 464.6338 - val_loss: 60.2482\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 536.9948 - val_loss: 72.6797\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 537.8254 - val_loss: 61.6997\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 564.7666 - val_loss: 51.3902\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 519.4695 - val_loss: 75.6702\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 502.2463 - val_loss: 104.9736\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 518.2223 - val_loss: 126.5904\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 483.5614 - val_loss: 141.0855\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 527.0087 - val_loss: 115.4935\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 614.0449 - val_loss: 54.8574\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 493.9770 - val_loss: 45.2423\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 496.0036 - val_loss: 37.4444\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 511.3013 - val_loss: 56.8427\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 445.2755 - val_loss: 132.4115\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 550.3610 - val_loss: 130.3539\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 476.6694 - val_loss: 109.1760\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 454.0679 - val_loss: 60.6513\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 451.6341 - val_loss: 59.3922\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 516.4899 - val_loss: 43.3680\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 436.0459 - val_loss: 35.8829\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 486.5244 - val_loss: 36.5646\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 460.9092 - val_loss: 49.1289\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 412.1326 - val_loss: 58.7471\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 545.0986 - val_loss: 68.9351\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 397.3437 - val_loss: 60.0900\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 425.8652 - val_loss: 44.8545\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 518.3433 - val_loss: 39.6841\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 447.5154 - val_loss: 43.1237\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 454.2003 - val_loss: 37.7612\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 540.9921 - val_loss: 61.6589\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 565.3427 - val_loss: 78.5690\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 450.3993 - val_loss: 52.6193\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 546.3904 - val_loss: 38.3080\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 483.1246 - val_loss: 38.2741\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 418.1222 - val_loss: 43.7931\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 494.7350 - val_loss: 48.5866\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 554.2087 - val_loss: 50.2380\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 430.8061 - val_loss: 65.9222\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 426.3665 - val_loss: 76.3597\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 500.0085 - val_loss: 52.8932\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 448.3091 - val_loss: 38.2807\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 503.5699 - val_loss: 38.3939\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 466.0675 - val_loss: 43.0514\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 454.2491 - val_loss: 51.5312\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 370.2877 - val_loss: 38.0977\n",
      "Epoch 00145: early stopping\n"
     ]
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "history =model.fit(x=X_train,y=y_train,validation_data=(X_test,y_test),batch_size=32,epochs=200,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 48)                624       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 464)               22736     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 464)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 368)               171120    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 368)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                29520     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 224,081\n",
      "Trainable params: 224,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.186065193776332\n",
      "6.172330059147435\n",
      "0.9087834996966879\n"
     ]
    }
   ],
   "source": [
    "predictions=model.predict(X_test) \n",
    "print(mean_absolute_error(y_test,predictions))\n",
    "print(np.sqrt(mean_squared_error(y_test,predictions)))\n",
    "\n",
    "print(r2_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('/home/l/prog/ds/Projects/home_price_pred/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(scaler,open('scaler.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
